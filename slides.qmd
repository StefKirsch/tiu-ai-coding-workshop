---
title: "Feel the Vibes: AI-assisted Coding"
format:
  revealjs:
    theme: default
    css:  styles-overrides.scss  
    slide-number: true
    progress: true
    toc: true
    toc-depth: 1
    incremental: false
    smaller: true               # global smaller font size
    scrollable: true
bibliography: references.bib
editor: visual
---

# Introduction and basics

## LLMs in a nutshell {.center}

```{=html}
<iframe 
  width="600"
  height="400"
  src="https://www.youtube.com/embed/LPZh9BOjkQs?si=ifls2CZBbVzfF9nM" 
  title="YouTube video player" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share
  referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
</iframe>
```
## Why are LLMs so good at coding?

-   Lots of **code** and **documentation** online
-   Relatively clear rules
-   ... but it is truly amazing.

## How well are LLMs doing right now?

-   LLMs and agents can do more and more complex tasks, see [Moore's Law for AI agents](https://theaidigest.org/time-horizons)
-   Reliability issues remain (more on that later)
-   Crucial to **understand**, **tweak** and **test** the code

## Some quick definitions

<details>

<summary>*Software in Research* VS *Research Software*</summary>

This is a very important distinction! Here is the definition by @gruenpeter_2021_5504016, that summarizes the results of the FAIR4RS working group:

> ‚Äú**Research Software** includes **source code files, algorithms, scripts, computational workflows and executables** that were **created during the research process** or **for a research purpose**.
>
> Software components (e.g., operating systems, libraries, dependencies, packages, scripts, etc) that are used **for research** but were **not created during or with a clear research intent** should be considered **software in research** and not Research Software.‚Äù

</details>

<details>

<summary>*AI‚Äëassisted Coding*‚ÄØVS‚ÄØ*Vibe‚ÄØCoding*</summary>

We want you to feel the vibes here, not surrender to them.

**AI‚Äëassisted coding** means using the AI as an assistant that helps you plan, write and understand code. You are still in the driver seat and have to make all important decisions. This is good, because it makes it easier (but not inevitable) that you understand what you actually produce.

**Vibe¬†Coding** goes further and delegates basically all decision-making to the AI. This can of course be very powerful, but it is basically impossible to understand what the code actually does and it can make it very challenging to fix bugs later on. Most importantly, it increases the risk of oversights that could then go completely unnoticed.

</details>

<iframe src="https://giphy.com/embed/yIh6EVwpd2VJ6NbNNY" width="480" height="269" frameborder="0" class="giphy-embed" allowfullscreen>

</iframe>

<details>

<summary>Tokens</summary>

A **token** is the model‚Äôs smallest chunk of text‚Äîtypically 3‚ÄØ‚Äì‚ÄØ4‚ÄØcharacters. "All" LLMs do is predict the most likely token to come next.

</details>

<details>

<summary>*Prompt* and *Output*</summary>

The **prompt** is what you add to the context to create the next piece of output. The **output** (completion) is the model‚Äôs response generated from that prompt.

</details>

<details>

<summary>Context</summary>

For longer, real‚Äëlife chats it‚Äôs often more useful to not think in prompts, but in the full **context**. The context is the full window of tokens the model can ‚Äúsee‚Äù, i.e. the chat history plus anything else the LLM provider adds to the context. LLMs have a limited context size, which means they will at some point "forget" what they saw earlier in the conversation.

</details>

## Plain LLM VS Reasoning Models

::: columns
::: column
### Standard LLM

```{=html}
<iframe src="https://giphy.com/embed/tTyTbFF9uEbPW"
        width="480" height="270"
        style="display:block; margin:0 auto;"
        frameborder="0" allowfullscreen>
</iframe>
```
<details>

<summary>More info</summary>

Standard models produce output by producing tokens. Their context is the entire previous conversation, including the response that is currently generated.

All logic (‚Äúthinking‚Äù) happens internally and ad‚Äëhoc.

</details>
:::

::: column
### Reasoning model

```{=html}
<iframe src="https://giphy.com/embed/WTDvmZyUDlngcu7q5L"
        width="480" height="270"
        style="display:block; margin:0 auto;"
        frameborder="0" allowfullscreen>
</iframe>
```
<details>

<summary>More info</summary>

Reasoning models have the ability **and** have been specifically trained to produce tokens in **two phases**.

1.  Thinking tokens: These serve as a scratchpad, for making a plan, noting things down for later, taking notes of what didn't work etc. The model is aware that this is not part of the output. You can inspect this as the user, even though proprietary providers will typically show you a summary of the thinking process and not the whole output.

2.  The user-facing output: The model considers this as the "actual" output. This phase is basically how non-reasoning models operate, but in this case, we have all the notes from the previous phase, which can help a lot in producing a better result.

</details>
:::
:::

## Plain LLM VS Reasoning Models

![](images/non-reasoning.png){width="900" style="max-height:unset;"}

## Plain LLM VS Reasoning Models

![](images/reasoning.png){width="900" style="max-height:unset;"}

## When to use which type of model

::: columns
::: column
### Standard LLM

-   "Small" questions (e.g. on syntax)
-   Whenever there is little context
-   Writing
-   ...
:::

::: column
### Reasoning models

-   Whenever the standard LLM fails
-   Suggesting case-specific solutions
-   Web search, research, exploration on new concepts
-   ...
:::
:::

::: callout-tip
## Best practice

Play around with different models in different cases and get a feeling for yourself.
:::

## What is an Agent then? {.incremental}

-   **Standard definition**: Something that has (1) **a goal** and (2) can take **actions in the real world**.
-   **Narrow definition**: A meta-LLM that can call other tools (including other LLMs), can access the command line and can write files.

::: callout-note
Modern reasoning LLMs (e.g. o3 and o4 series) have agentic capabilities built in.
:::

## Ways of interacting with an LLM (1/2)

::: columns
::: column
### Chatbot

![](https://images.seeklogo.com/logo-png/50/1/chatgpt-logo-png_seeklogo-503286.png){width="30%"} ![](https://images.seeklogo.com/logo-png/61/1/microsoft-copilot-logo-png_seeklogo-619562.png){width="30%"} ![](https://images.seeklogo.com/logo-png/55/1/claude-logo-png_seeklogo-554540.png){width="30%"}

-   Tops ‚úÖ
    -   Model choice
    -   Iterate on prompts
    -   Lots of control (including which data you enter -\> *GDPR compliance*)
-   Flops ‚ùå
    -   Copy pasting
    -   No further automation
    -   Only aware of the context you give it
:::

::: column
### (API)

-   Not really relevant for coding
:::
:::

## Ways of interacting with an LLM (2/2)

::: columns
::: column
### IDE plugins

![](https://images.seeklogo.com/logo-png/40/1/github-copilot-logo-png_seeklogo-407787.png){width="30%"} ![](https://analyticsindiamag.com/wp-content/uploads/2025/04/windsurf-featured-1300x731.png){width="45%"}

-   Tops ‚úÖ
    -   Automation
    -   Context-aware of your codebase
    -   Different modes of interaction (autocomplete, chat)
-   Flops ‚ùå
    -   No control over what gets transmitted -\> critical for GDPR and IP
    -   Can get expensive for API calls
:::

::: column
### Agents

![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*clU4bA8DttJiyPnrRBVLWw.png){width="45%"}

![](https://i.ytimg.com/vi/oB4JR98KRAA/mqdefault.jpg){width="45%"}

-   Goal + write access to your codebase
-   Automatically calls API with prompts.
:::
:::

## Prompt "engineering"

A little overhyped...

<details>

<summary>More info</summary>

**Prompt engineering** is important when you ask an LLM for a complete solution or when you develop an application that routinely talks to an LLM API. In those cases phrasing and "strategic" considerations can make a big difference.

For day-to-day use and especially AI-assisted coding, you typically don't need to overthink (engineer) your prompts as long as you keep in mind a couple of things.

</details>

### ü§î What to ask for

-   Mindset: Ask for advice, not for solutions
-   Either very broad or very specific
-   Ask for a battle plan
    -   Think in building blocks afterwards
    -   Assemble them yourself
-   Ask advice on how to do it well
-   Ask if you overlooked anything

### üí° How to ask

-   Tell it all it needs to know
    -   It can't read your mind, it doesn't know anything about you or your project
    -   ... and even if it does, it probably it doesn't know what that means
-   "How do I xyz?" instead of "Do xyz for me."
-   Be specific in your requirements (you can discuss them before with AI)
-   Your role: Product manager and quality assurance

## Typical tasks

-   I am using framework/package/context X. How do I do... (e.g. Jupyter notebook, R tidyverse)

-   Research packages and frameworks that already do what you need (FAIR)

-   Refactoring

-   Generate synthetic data

-   Feedback on existing code (context & style becomes super important here to not get lost on a tangent)

## Questions? {.center style="text-align:center;"}

# Applicable Guidelines and Regulations

## What is AI anyway

![](https://media1.tenor.com/m/fHz7wfvakO8AAAAd/theyre-the-same-picture-the-office.gif){width="900" style="max-height:unset;"}

<details>

<summary>The EU AI Act has a very broad definition</summary>

> ‚ÄòAI system‚Äô means a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments;

</details>

## Why is AI use potentially problematic?

::: columns
::: column
### üîí Information, knowledge and data safety

-   Computationally heavy ‚Üí Need to run in the cloud
-   Not European
-   New

<details>

<summary>Is AI really special?</summary>

All data protection considerations have to be made regardless of the fact whether a given system is considered **AI or not**. There is a common misconception that AI itself is the problem, e.g. because the models are trained on the prompts. **That is not correct**. Instead, the issue lies with **cloud-based systems** for which your institution/employer does not have a **data processing agreement** with the supplier.

</details>
:::

::: column
### üéì Integrity

-   GenAI will happily attempt to do anything
-   Obscure failure modes
    -   Hallucinations
    -   Lack of self-criticism
-   Plagiarism question
-   More material can be created faster
    -   Bigger chance of mistakes
    -   Greater potential for forgery
:::
:::

## Guiding Principles

-   Adherence to local guidelines and checklists
-   Compliance with national and EU legislation
-   Research integrity and clear end responsibility
-   Rules of funders and journals

```{=html}
<iframe src="https://giphy.com/embed/ZikyVyLF7aEaQ" width="480" height="360" style="" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
```
## University guidelines

General

-   [Information security policy](https://www.tilburguniversity.edu/about/conduct-and-integrity/privacy-and-security/information-security)
-   [Knowledge security](https://www.tilburguniversity.edu/about/conduct-and-integrity/privacy-and-security/knowledge-security)
-   [Data protection](https://www.tilburguniversity.edu/about/conduct-and-integrity/privacy-and-security/privacy-policy)

GPAI specific

-   [AI guidelines for researchers](https://www.tilburguniversity.edu/intranet/organization-policy/strategy/infrastructure/research-infrastructure/ai-guidelines-researchers)

<details>

<summary>Guidelines are there to help you</summary>

In practice, it can be quite challenging to be and stay up to date with the latest regulations. University guidelines should be viewed as a practical interpretation for you as the end user. Unfortunately, the university can't relieve you from your legal and ethical responsibilities, so you should do your due diligence and check which rules and regulations apply in your situation.

</details>

::: callout-tip
## Ask for advice

Talk to your school's information manager, data steward or the central Research Data Office for support.
:::

## National and EU law

-   GDPR
-   Intellectual property rights (for text, data and software)
-   EU AI Act

## Research integrity

<details>

<summary>Just as any research output, the same principles also apply to **code**!</summary>

In the past, **Research Software** has often been considered as a necessary intermediate product of research, but was often not worth mentioning or publishing. Times have changed though, and both Tilburg University, but also many funders and journals consider **Research Software** as a relevant and crucial output that is worth highlighting and discussing. This is for two reasons:

1.  From an Open Science and FAIR perspective, sharing your code creates the necessary transparency on how you got to your results, so that your research becomes trustworthy.
2.  Universities, funders and journals have noticed that we still often reinvent the wheel for new projects. Creating software that is easy to reuse in other projects can thus be enormously beneficial in advancing the science faster and significantly cheaper.

Software has been contributing to the results of published research for a long time, and has therefore always been subject to research integrity principles. In modern times though, we see more and more that integrity in software development is not only assumed anymore, but also tested for more and more.

</details>

In particular relevant for GenAI use:

-   No fraudulent activities

    -   Data fabrication
    -   Manipulation of data, analyses and results

-   Ensure reproducibility and transparency

    -   [Data Handling and Methods Reporting](https://www.tilburguniversity.edu/research/social-and-behavioral-sciences/science-committee) (TSB)
    -   [Guideline Replication Package](https://www.tilburguniversity.edu/research/economics-and-management/replication-package) (TiSEM)

## Rules of funders and journals

NWO

-   Generative AI allowed for applicants

Journals

-   GenAI policies vary a lot, from ... to:

    -   No restriction on AI use at all

    -   Require statement on AI use

    -   Exact list of AI models and prompts as soon as it touches data *or* when used to develop code and the analysis methods

-   Most journals don't have explicit policy around AI-assisted creation of code and analyses.

::: callout-tip
## Our Recommendation

-   Stick to the most strict guidelines of relevant funders & journals in your field

-   For each funder and journal, check what they mean by 'AI'.

-   Don't bootstrap your own way of doing things. Look up best practices and ask for advice!
:::

## Common compliance slips

::: callout-important
## GDPR also applies to publicly available data

The GDPR makes no distinction between publicly available and non-publicly available data. That means that personal data, even if found online, should not be entered into cloud-based (AI) systems.
:::

::: callout-important
## Data transfer via automated systems

Be careful with tools that interact with an API by themselves and where you can't inspect and approve the prompts yourself. They might transfer sensitive data from your working directory without your knowledge.

Examples are: Windsurf, GitHub Copilot and Agentic development tools.
:::

## Questions? {.center style="text-align:center;"}

# Best Practices

## üòµ Which model do I use?

::: columns
::: {.column width="50%"}
*How the UI looked before*

![](images/choice_of_models.png){height="50%"}
:::

::: {.column width="50%"}
-   Different models for different tasks.

-   4o series for writing/ planning.

-   o3, o4 mini and o4 mini high for coding and logical reasoning.
:::
:::

## üòµ Which model do I use?

![](images/GPT5.png)

-   Picks the appropriate model based on the task at hand!\*

<details>

<summary>\*More about this</summary>

-   GPT5 may not always do this. It is at best, using key words in your prompt to gauge task complexity (and deciding further course of action)
-   Sometimes it will think way too little and sometimes start reasoning about something it doesn't need reasoning for.
-   It is always an advantage to know a bit about the different models. This video tells you about them in detail and with use-cases.

<iframe width="560" height="315" src="https://www.youtube.com/embed/uluRM4yIras?si=3caDpUQO9zF904OO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>

</iframe>

</details>

## üë• Using version control and Git

**Without Git**

::: columns
::: {.column width="50%"}
::: columns

<iframe src="https://giphy.com/embed/2KAGlmkPywhZS" width="247" height="480" frameborder="0" allowfullscreen>

</iframe>
:::
:::

::: {.column width="50%"}

::: columns
##### Summary: nightmare üëπ

-   Difficulty maintaining versions.

-   No clue which version contains what changes.
:::
:::
:::

## üë• Using version control and Git

::: columns
::: {.column width="50%"}
**With git:**

![](https://i.imgflip.com/a64www.jpg){width="700" style="max-height:unset;"}
:::

::: {.column width="50%"}
##### Summary: *struggle* but for the right reasons ‚õπÔ∏è‚Äç‚ôÇÔ∏è

-   Git keeps every change safe and rewindable.

```{=html}
<!-- -->
```
-   You can always fall back to a working version.
:::
:::

##### 

<details>

<summary>Learn more about git and how you can use it</summary>

-   [A fantastic guide to using git and github](https://www.youtube.com/watch?v=RGOj5yH7evk&t=360s) üçè
-   [What is Gitlab and how it may be a better choice than Github](https://www.incredibuild.com/blog/gitlab-vs-github-comparison)?‚öñ

</details>

## Using `git diff`

The command `git diff` tells you exactly what changed in your code.

::: callout-warning
# `git diff` and AI written code

-   Using `git diff` is crucial when using ChatGPT to write code.
-   Even with a near-perfect prompting, ChatGPT can create completely unrelated changes in your code.
:::

![](images/git_diff_1.png)

![](images/git_diff_2.png)

<details>

<summary>More on git diff</summary>

-   [Datacamp article on git diff](https://www.datacamp.com/tutorial/git-diff-guide)

</details>

## üö´ How to (not) have it write code

::: callout-note
## üèÜ Golden Rule 1

**It is your responsibility to have enough context into the problem. The lesser context you have, the less effective it might be in the long run.**
:::

![](images/GPT5_context_1.png)

## Thus began the cycle of debugging ChatGPTs code

-   Code provided doesn't run locally:

![](images/GPT5_context_2.png)

-   Features that work in canvas fail in my machine:

![](images/GPT5_context_3.png)

## End result of such workflows

-   You end up debugging ChatGPTs code way more than you would like.

-   Takes longer overall (even if the start was quick)

![](https://i.imgflip.com/a64vaj.jpg){width="600" style="max-height:unset;"}

## ‚úÖ How to have it write code

**1. Narrow the scope of the problem**

::: callout-note
## üèÜ Golden Rule 2

**Narrow and specific problems enable you to have more control and understanding of your own code and the AI generated code.**
:::

![](images/GPT5_specific_problem_1.png){height="30%"}

## 

![](images/GPT5_specific_problem_2.png){height="15%"}

## 

**2. üí° Understanding code with it**

-   Breaking down unfamiliar code

![](images/GPT5_understand_code_1.png)

## 

-   Improving your own code base: general feedback

![](images/GPT5_understand_code_2.png)

-   A glimpse of the response: ![](images/GPT5_understand_code_3.png)

## 

-   Improving your own code base: with use-case

![](images/GPT5_understand_code_4.png){width="30%"}

## üìì Takeaways for AI powered code reviews

-   ChatGPT has very high standards (looks into both major and very minor issues) - take feedback with a grain of salt.
-   More context of use-case: better suited responses
-   Keep re-reading the first point until you are convinced üéà

![](https://i.imgflip.com/a64urh.jpg){width="500" style="max-height:unset;"}

## üìè Prompt engineering - verbosity

-   Limit verbosity (in general), you can always dig for more if *you* want

![](images/GPT5_prompt_engineering_1.png)

## üìè Prompt engineering - verbosity

![](images/GPT5_prompt_engineering_2.png){width="500"}

## üìè Prompt engineering - clarity over conciseness

-   Asking to focus on clarity over conciseness

![](images/GPT5_prompt_engineering_3.png){width="500"}

## üìè Prompt engineering - clarity over conciseness

![](images/GPT5_prompt_engineering_4.png){width="500"}

## üìè Prompt engineering - assigning roles

-   On assigning roles and more

    <iframe width="560" height="315" src="https://www.youtube.com/embed/X7YjqKk-7Y0?si=hYKV42m7p6FMOUWD" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>

    </iframe>

<details>

<summary>More examples with resources</summary>

1.  [Ethan Mollick on prompt engineering](https://www.moreusefulthings.com/instructor-prompts)
2.  [TU Delft's wiki on prompting ChatGPT for researchers](https://hri-wiki.tudelft.nl/llm/chat-gpt-prompts)

</details>

## üôÑ Do the right thing

Look up specific packages that are able to accomplish your task before completely pinning it on ChatGPT. This is safer and can also be more accurate.

Examples:

1.  [OpenAI whisper](https://github.com/openai/whisper)- *transcribes audio files  offline*

2.  [textwash](https://github.com/ben-aaron188/textwash/tree/main) - *anonymize text offline*

![](https://i.imgflip.com/a5je7z.jpg){width="500" style="max-height:unset;"}

## ‚òÉÔ∏è When to take a break

-   *Subtle* signs of frustration

::: columns
::: {.column width="70%"}
![](images/GPT5_frustration_1.png)
:::

::: {.column width="30%"}
<iframe src="https://giphy.com/embed/y1WDIwAZRSmru" width="400" height="300" style frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>
:::
:::

::: columns
::: {.column width="60%"}
-   Collaboration turns into dependence: ***your problem is still yours to solve; with or without it***
:::

::: {.column width="40%"}
<iframe src="https://giphy.com/embed/4foAPrZ3GVzWM" width="480" height="240" style frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>
:::
:::

## üèÅ Using Projects

![](images//Projects_callout.png){width="900" style="max-height:unset;"}

::: callout-tip
## Using Projects

I have found using projects to be a fantastic aid in managing my chats and using them mindfully.

-   Add specific context or media that ChatGPT will keep in its memory for future chats.
-   Better workflow for different projects with corresponding chats
:::

## Questions? {.center style="text-align:center;"}

# Pitfalls

## Pitfall 1 - Taking shortcuts

<details>

<summary>Shortcuts often turn out costly in the end.</summary>

It is very tempting to just ask the AI to do a whole work package for you, especially when you are under time pressure. Even worse still, the code will probably work, which hides the potential cost in the long run.

The result is often code that doesn't fit the logic and style of the rest of your project, adds unnecessary dependencies or in some other way does not fit well into the bigger picture.

</details>

![](https://i.imgflip.com/a4v6za.jpg)

::: callout-tip
## Our Recommendation

Find a workflow that balances *efficiency* and *sustainability* and that works for you. Then stick to it.
:::

## Pitfall 2 - Punching above your weight

::: {style="text-align:center"}
![](images/dum_dum.png){width="600" style="max-height:unset;"}
:::

<details>

<summary>Only ask AI to do what you could also do yourself.</summary>

The sustainable route of using AI is to not have it write code that you could not have produced yourself. This of course doesn't mean that you can't ask AI **help you** write that code!

::: callout-tip
## Our Recommendation

We recommend that you only (fully) outsource tasks to AI that are basically trivial for you.
:::

</details>

## Pitfall 3 - Context conundrums

::: columns
::: column
The context in the example of ChatGPT

-   The system prompt

-   Custom instructions

-   "Memory"

-   Project context

-   ???

-   Your prompts and the model's responses (the conversation)
:::

::: column
![](https://i.imgflip.com/a4uupf.jpg){width="500" style="max-height:unset;"}
:::
:::

## Pitfall 3a - Context window overflow

<details>

<summary>LLMs can only remember so much</summary>

Especially in longer conversations, you can quickly exceed the functional context window, which is often much less than the nominal ones (e.g., 10k words for ChatGPT 5).

This means that the model will forget what was said earlier, especially if it was somewhere in the middle of the conversation.

</details>

![](https://i.imgflip.com/a4v4gn.jpg){width="500" style="max-height:unset;"}

## Pitfall 3b - Muddy the (context) water

<details>

<summary>A little context hygiene goes a long way.</summary>

Many of the advertised features, like "custom instructions," "memory," and "projects," boil down to adding extra context to the conversation history (aka context stuffing). In most cases, this is helpful and feels like genuine personalization that adds convenience. But it can also lead to strange outcomes when the AI can't quite decide which bit of the context is actually relevant at a given time. It can also change the behavior of the model in unexpected ways.

Unfortunately, all the extra context is typically hidden from the user, at least while chatting with the model. At the time of writing, however, most providers allow you to inspect, modify, or delete unwanted context fluff, albeit it being scattered across various corners of the UI. A quick web search or asking the AI of choice can typically help you identify and remove the culprits, though.

</details>

![](https://i.imgflip.com/a4v36z.jpg){width="500" style="max-height:unset;"}

## Pitfall 5: Understand it or lose it

![](images/pitfalls_documentation.png){width="1000"}

-   **Learn before you lean^1^**: Understanding AI written code is the ***only*** way to write code with AI.

-   **Documentation is your best friend.** Writing comments for your code (what it does and why you chose a certain strategy) is the safest way to make it maintainable.

<iframe src="https://giphy.com/embed/KyGiMJokZEQvu" width="480" height="269" style frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>

<details>

<summary>Sources</summary>

1.  [The GenAI 'crutch': why teams must learn before they lean \| TechRadar](https://www.techradar.com/pro/the-genai-crutch-why-teams-must-learn-before-they-lean?utm_source=chatgpt.com)
2.  [Risks of vibe coding](https://www.techradar.com/pro/from-vibe-to-viable-the-hidden-cost-of-ai-tech-debt?utm_source=chatgpt.com)

</details>

## Questions? {.center style="text-align:center;"}

## References

::: {#refs}
:::
