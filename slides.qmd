---
title: "Feel the Vibes: AI-assisted Coding"
format:
  revealjs:
    theme: default
    css:  styles-overrides.scss  
    slide-number: true
    progress: true
    toc: true
    toc-depth: 1
    incremental: false
    smaller: true               # global smaller font size
    scrollable: true
bibliography: references.bib
editor: visual
---

# Introduction and basics

## LLMs in a nutshell {.center}

```{=html}
<iframe 
  width="600"
  height="400"
  src="https://www.youtube.com/embed/LPZh9BOjkQs?si=ifls2CZBbVzfF9nM" 
  title="YouTube video player" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share
  referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
</iframe>
```

## Why are LLMs so good at coding?

-   Lots of **code** and **documentation** online
-   Relatively clear rules
-   ... but it is truly amazing.

## How well are LLMs doing right now?

-   LLMs and agents can do more and more complex tasks, see [Moore's Law for AI agents](https://theaidigest.org/time-horizons)
-   Reliability issues remain (more on that later)
-   Crucial to **understand**, **tweak** and **test** the code

## Some quick definitions

<details markdown="block">
<summary markdown="span">*Software in Research* VS *Research Software*</summary>

This is a very important distinction! Here is the definition by @gruenpeter_2021_5504016, that summarizes the results of the FAIR4RS working group:

> ‚Äú**Research Software** includes **source code files, algorithms, scripts, computational workflows and executables** that were **created during the research process** or **for a research purpose**. 
> 
> Software components (e.g., operating systems, libraries, dependencies, packages, scripts, etc) that are used **for research** but were **not created during or with a clear research intent** should be considered **software in research** and not Research Software.‚Äù

</details>


<details markdown="block">
<summary markdown="span">*AI‚Äëassisted Coding*‚ÄØVS‚ÄØ*Vibe‚ÄØCoding*</summary>

We want you to feel the vibes here, not surrender to them.

**AI‚Äëassisted coding** means using the AI as an assistant that helps you plan, write and understand code. You are still in the driver seat and have to make all important decisions. This is good, because it makes it easier (but not inevitable) that you understand what you actually produce.

**Vibe¬†Coding** goes further and delegates basically all decision-making to the AI. This can of course be very powerful, but it is basically impossible to understand what the code actually does and it can make it very challenging to fix bugs later on. Most importantly, it increases the risk of oversights that could then go completely unnoticed.

</details>

<iframe src="https://giphy.com/embed/yIh6EVwpd2VJ6NbNNY"
  width="480"
  height="269"
  frameborder="0"
  class="giphy-embed"
  allowfullscreen>
</iframe>

<details markdown="block">
<summary markdown="span">Tokens</summary>

A **token** is the model‚Äôs smallest chunk of text‚Äîtypically 3‚ÄØ‚Äì‚ÄØ4‚ÄØcharacters. "All" LLMs do is predict the most likely token to come next.

</details>

<details markdown="block">
<summary markdown="span">*Prompt* and *Output*</summary>

The **prompt** is what you add to the context to create the next piece of output. 
The **output** (completion) is the model‚Äôs response generated from that prompt.

</details>

<details markdown="block">
<summary markdown="span">Context</summary>

For longer, real‚Äëlife chats it‚Äôs often more useful to not think in prompts, but in the full **context**. The context is the full window of tokens the model can ‚Äúsee‚Äù, i.e. the chat history plus anything else the LLM provider adds to the context.
LLMs have a limited context size, which means they will at some point "forget" what they saw earlier in the conversation.

</details>


## Plain LLM VS Reasoning Models

::::: columns

::: column

### Standard LLM

```{=html}
<iframe src="https://giphy.com/embed/tTyTbFF9uEbPW"
        width="480" height="270"
        style="display:block; margin:0 auto;"
        frameborder="0" allowfullscreen>
</iframe>
```

<details markdown="block">
<summary markdown="span">More info</summary>

Standard models produce output by producing tokens. Their context is the entire previous conversation, including the response that is currently generated.

All logic (‚Äúthinking‚Äù) happens internally and ad‚Äëhoc.
</details>

:::

::: column

### Reasoning model

```{=html}
<iframe src="https://giphy.com/embed/WTDvmZyUDlngcu7q5L"
        width="480" height="270"
        style="display:block; margin:0 auto;"
        frameborder="0" allowfullscreen>
</iframe>
```

<details markdown="block">
<summary markdown="span">More info</summary>

Reasoning models have the ability **and** have been specifically trained to produce tokens in **two phases**.

1. Thinking tokens: These serve as a scratchpad, for making a plan, noting things down for later, taking notes of what didn't work etc. The model is aware that this is not part of the output. You can inspect this as the user, even though proprietary providers will typically show you a summary of the thinking process and not the whole output.

2. The user-facing output: The model considers this as the "actual" output. This phase is basically how non-reasoning models operate, but in this case, we have all the notes from the previous phase, which can help a lot in producing a better result.

</details>



:::

:::::

## Plain LLM VS Reasoning Models

![](images/non-reasoning.png){width="900" style="max-height:unset;"} 

## Plain LLM VS Reasoning Models

![](images/reasoning.png){width="900" style="max-height:unset;"} 

## When to use which type of model

:::: columns

:::column
### Standard LLM

-   "Small" questions (e.g. on syntax)
-   Whenever there is little context
-   Writing
-   ...

:::

:::column
### Reasoning models

-   Whenever the standard LLM fails
-   Suggesting case-specific solutions
-   Web search, research, exploration on new concepts
-   ...

:::

::::

::: {.callout-tip}
## Best practice

Play around with different models in different cases and get a feeling for yourself.
:::

## What is an Agent then? {.incremental}

-   **Standard definition**: Something that has (1) **a goal** and (2) can take **actions in the real world**.
-   **Narrow definition**: A meta-LLM that can call other tools (including other LLMs), can access the command line and can write files.

::: {.callout-note}
Modern reasoning LLMs (e.g. o3 and o4 series) have agentic capabilities built in.
:::

## Ways of interacting with an LLM (1/2)

::::: columns
::: column
### Chatbot

![](https://images.seeklogo.com/logo-png/50/1/chatgpt-logo-png_seeklogo-503286.png){width="30%"} ![](https://images.seeklogo.com/logo-png/61/1/microsoft-copilot-logo-png_seeklogo-619562.png){width="30%"} ![](https://images.seeklogo.com/logo-png/55/1/claude-logo-png_seeklogo-554540.png){width="30%"}

-   Tops ‚úÖ
    -   Model choice
    -   Iterate on prompts
    -   Lots of control (including which data you enter -\> *GDPR compliance*)
-   Flops ‚ùå
    -   Copy pasting
    -   No further automation
    -   Only aware of the context you give it
:::

::: column
### (API)

-   Not really relevant for coding
:::
:::::

## Ways of interacting with an LLM (2/2)

::::: columns
::: column
### IDE plugins

![](https://images.seeklogo.com/logo-png/40/1/github-copilot-logo-png_seeklogo-407787.png){width="30%"} ![](https://analyticsindiamag.com/wp-content/uploads/2025/04/windsurf-featured-1300x731.png){width="45%"}

-   Tops ‚úÖ
    -   Automation
    -   Context-aware of your codebase
    -   Different modes of interaction (autocomplete, chat)
-   Flops ‚ùå
    -   No control over what gets transmitted -\> critical for GDPR and IP
    -   Can get expensive for API calls
:::

::: column
### Agents

![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*clU4bA8DttJiyPnrRBVLWw.png){width="45%"}

![](https://i.ytimg.com/vi/oB4JR98KRAA/mqdefault.jpg){width="45%"}

-   Goal + write access to your codebase
-   Automatically calls API with prompts.
:::
:::::

## Prompt "engineering"

A little overhyped...

<details markdown="block">
<summary markdown="span">More info</summary>

**Prompt engineering** is important when you ask an LLM for a complete solution or when you develop an application that routinely talks to an LLM API. In those cases phrasing and "strategic" considerations can make a big difference. 

For day-to-day use and especially AI-assisted coding, you typically don't need to overthink (engineer) your prompts as long as you keep in mind a couple of things.

</details>

### ü§î What to ask for

-   Mindset: Ask for advice, not for solutions
-   Either very broad or very specific
-   Ask for a battle plan 
    -   Think in building blocks afterwards 
    - Assemble them yourself
-   Ask advice on how to do it well
-   Ask if you overlooked anything

### üí° How to ask

-   Tell it all it needs to know
    -   It can't read your mind, it doesn't know anything about you or your project
    -   ... and even if it does, it probably it doesn't know what that means
-   "How do I xyz?" instead of "Do xyz for me."
-   Be specific in your requirements (you can discuss them before with AI)
-   Your role: Product manager and quality assurance

## Typical tasks

-   I am using framework/package/context X. How do I do... (e.g. Jupyter notebook, R tidyverse)

-   Research packages and frameworks that already do what you need (FAIR)

-   Refactoring

-   Generate synthetic data

-   Feedback on existing code (context & style becomes super important here to not get lost on a tangent)


## Questions? {.center style="text-align:center;"}

# Applicable Guidelines and Regulations

## What is AI anyway

![](https://media1.tenor.com/m/fHz7wfvakO8AAAAd/theyre-the-same-picture-the-office.gif){width="900" style="max-height:unset;"} 

<details markdown="block">
<summary markdown="span">The EU AI Act has a very broad definition</summary>

> ‚ÄòAI system‚Äô means a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments;

</details>

## Why is AI use potentially problematic?

::::columns
:::column
### üîí Information, knowledge and data safety

-   Computationally heavy &rarr; Need to run in the cloud
-   Not European
-   New

<details markdown="block">
<summary markdown="span">Is AI really special?</summary>

All data protection considerations have to be made regardless of the fact whether a given system is considered **AI or not**. There is a common misconception that AI itself is the problem, e.g. because the models are trained on the prompts. **That is not correct**. Instead, the issue lies with **cloud-based systems** for which your institution/employer does not have a **data processing agreement** with the supplier. 

</details>

:::


:::column
### üéì Integrity

-   GenAI will happily attempt to do anything
-   Obscure failure modes
    -   Hallucinations
    -   Lack of self-criticism
-   Plagiarism question
-   More material can be created faster
    -   Bigger chance of mistakes
    -   Greater potential for forgery

:::


::::

## Guiding Principles

-   Adherence to local guidelines and checklists
-   Compliance with national and EU legislation
-   Research integrity and clear end responsibility
-   Rules of funders and journals


```{=html}
<iframe src="https://giphy.com/embed/ZikyVyLF7aEaQ" width="480" height="360" style="" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
```

## University guidelines

General

-   [Information security policy](https://www.tilburguniversity.edu/about/conduct-and-integrity/privacy-and-security/information-security)
-  [Knowledge security](https://www.tilburguniversity.edu/about/conduct-and-integrity/privacy-and-security/knowledge-security)
-   [Data protection](https://www.tilburguniversity.edu/about/conduct-and-integrity/privacy-and-security/privacy-policy)

GPAI specific

-   [AI guidelines for researchers](https://www.tilburguniversity.edu/intranet/organization-policy/strategy/infrastructure/research-infrastructure/ai-guidelines-researchers)

<details markdown="block">
<summary markdown="span">Guidelines are there to help you</summary>

In practice, it can be quite challenging to be and stay up to date with the latest regulations. University guidelines should be viewed as a practical interpretation for you as the end user. Unfortunately, the university can't relieve you from your legal and ethical responsibilities, so you should do your due diligence and check which rules and regulations apply in your situation. 

</details>

::: {.callout-tip}
## Ask for advice

Talk to your school's information manager, data steward or the central Research Data Office for support.

:::

## National and EU law

-   GDPR
-   Intellectual property rights (for text, data and software)
-   EU AI Act


## Research integrity

<details markdown="block">
<summary markdown="span">Just as any research output, the same principles also apply to **code**!</summary>

In the past, **Research Software** has often been considered as a necessary intermediate product of research, but was often not worth mentioning or publishing. Times have changed though, and both Tilburg University, but also many funders and journals consider **Research Software** as a relevant and crucial output that is worth highlighting and discussing. This is for two reasons:

1. From an Open Science and FAIR perspective, sharing your code creates the necessary transparency on how you got to your results, so that your research becomes trustworthy.
2. Universities, funders and journals have noticed that we still often reinvent the wheel for new projects. Creating software that is easy to reuse in other projects can thus be enormously beneficial in advancing the science faster and significantly cheaper.

Software has been contributing to the results of published research for a long time, and has therefore always been subject to research integrity principles. In modern times though, we see more and more that integrity in software development is not only assumed anymore, but also tested for more and more. 

</details>

In particular relevant for GenAI use:

- No fraudulent activities
    -   Data fabrication
    -   Manipulation of data, analyses and results

- Ensure reproducibility and transparency

    -   [Data Handling and Methods Reporting](https://www.tilburguniversity.edu/research/social-and-behavioral-sciences/science-committee) (TSB)
    -   [Guideline Replication Package](https://www.tilburguniversity.edu/research/economics-and-management/replication-package) (TiSEM)

## Rules of funders and journals

NWO

-   Generative AI allowed for applicants

Journals

-   GenAI policies vary a lot, from ... to:

    -   No restriction on AI use at all

    -   Require statement on AI use

    -   Exact list of AI models and prompts as soon as it touches data *or* when used to develop code and the analysis methods

-   Most journals don't have explicit policy around AI-assisted creation of code and analyses.

::: {.callout-tip}
## Our Recommendation

-   Stick to the most strict guidelines of relevant funders & journals in your field

-   For each funder and journal, check what they mean by 'AI'.

-   Don't bootstrap your own way of doing things. Look up best practices and ask for advice!



:::

## Common compliance slips

::: {.callout-important}
## GDPR also applies to publicly available data

The GDPR makes no distinction between publicly available and non-publicly available data. That means that personal data, even if found online, should not be entered into cloud-based (AI) systems.

:::


::: {.callout-important}
## Data transfer via automated systems

Be careful with tools that interact with an API by themselves and where you can't inspect and approve the prompts yourself. They might transfer sensitive data from your working directory without your knowledge.

Examples are: Windsurf, GitHub Copilot and Agentic development tools.
:::

# Pitfalls

## Pitfall 1 - Taking shortcuts

Shortcuts often turn out costly in the end.

![](https://i.imgflip.com/a4v6za.jpg)

Find a workflow that balances efficiency and sustainability and that works for you. Then stick to it.

## Pitfall 2 - Punching above your weight

::::: columns
::: column
![](images/dum_dum.png){width="500" style="max-height:unset;"}
:::

::: column
```{=html}
<iframe src="https://giphy.com/embed/4559yWLyv3huw" width="480" height="269" style="" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
```
:::
:::::

<details>

<summary>Only ask it to do what you could also do yourself.</summary>

The sustainable route of using AI is to not have it write code that you could not have produced yourself. This of course doesn't mean that you can't ask AI **help you** write that code!

We recommend that you only (fully) outsource tasks to AI that are basically trivial fo you to do.

</details>

## Pitfall 3 - Context conundrums

::::: columns
::: column
The context in the example of ChatGPT

-   The system prompt 

-   Custom instructions

-   "Memory" 

-   Project context 

-   ??? 

-   Your prompts and the model's responses (the conversation)
:::
::: column
![](https://i.imgflip.com/a4uupf.jpg){width="500" style="max-height:unset;"}
:::
:::::




## Pitfall 3a - Context window overflow


<details>

<summary>LLMs can only remember so much</summary>

Especially in longer conversations, you can quickly exceed the functional context window, which is often much less than the nominal ones (e.g., 10k words for ChatGPT 5).

This means that the model will forget what was said earlier, especially if it was somewhere in the middle of the conversation.

</details>

![](https://i.imgflip.com/a4v4gn.jpg){width="500" style="max-height:unset;"}

## Pitfall 3b - Muddy the (context) water

<details>

<summary>A little context hygiene goes a long way.</summary>

Many of the advertised features, like "custom instructions," "memory," and "projects," boil down to adding extra context to the conversation history (aka context stuffing). In most cases, this is helpful and feels like genuine personalization that adds convenience. But it can also lead to strange outcomes when the AI can't quite decide which bit of the context is actually relevant at a given time. It can also change the behavior of the model in unexpected ways.

Unfortunately, all the extra context is typically hidden from the user, at least while chatting with the model. At the time of writing, however, most providers allow you to inspect, modify, or delete unwanted context fluff, albeit it being scattered across various corners of the UI. A quick web search or asking the AI of choice can typically help you identify and remove the culprits, though.

</details>

![](https://i.imgflip.com/a4v36z.jpg){width="500" style="max-height:unset;"}

## References

::: {#refs}
:::
